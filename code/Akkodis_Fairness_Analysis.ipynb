{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "%pip install pandas matplotlib seaborn scikit-learn openpyxl tensorflow xgboost aif360\n",
        "%pip install \"aif360[Reductions, inFairness]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuNMyoGU1ExN",
        "outputId": "d5147e88-16d9-4b30-8bc3-48067a5f22d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n",
            "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n",
            "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
            "pip install 'aif360[inFairness]'\n",
            "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
            "pip install 'aif360[Reductions]'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from scipy.stats import chi2_contingency, fisher_exact\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential # type: ignore\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization # type: ignore\n",
        "\n",
        "from aif360.datasets import BinaryLabelDataset, StandardDataset\n",
        "from aif360.algorithms.preprocessing import LFR\n",
        "from fairlearn.preprocessing import CorrelationRemover\n",
        "from aif360.algorithms.inprocessing import GerryFairClassifier, PrejudiceRemover, MetaFairClassifier\n",
        "from aif360.algorithms.postprocessing import EqOddsPostprocessing, RejectOptionClassification\n",
        "\n",
        "from fairlearn.metrics import MetricFrame\n",
        "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference, selection_rate, false_positive_rate, false_negative_rate, count\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "random_seed = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH = 'C:/Users/aberti/Desktop/ProjectWork_AEQUITAS_AKKODIS/data/'\n",
        "df = (\n",
        "    pd.read_excel(PATH + 'Dataset_2.0_Akkodis.xlsx')\n",
        "      .rename(columns=lambda c: c.lstrip().title())\n",
        ")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nh5kHyh1ExO"
      },
      "source": [
        "### Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop_duplicates(subset='Id', keep='last')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpIFJx6X1ExQ"
      },
      "outputs": [],
      "source": [
        "unuseful_columns = [\n",
        "    'ID', 'TAG', 'Year of insertion', 'Year of Recruitment', 'Recruitment Request', \n",
        "    'Assumption Headquarters', 'event_type__val', 'linked_search__key', 'Job Description', \n",
        "    'Candidate Profile', 'Akkodis headquarters', 'Standing/Position', 'Unnamed: 0', \n",
        "    'Residence', 'Last Role', 'Study Area.1', 'Years Experience.1']\n",
        "df = df.drop(columns=unuseful_columns)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "THRESHOLD = 0.4\n",
        "unuseful_columns = []\n",
        "for col in df.columns:\n",
        "  null_count = df[col].isna().sum() / df.shape[0]\n",
        "  if null_count > THRESHOLD:\n",
        "    unuseful_columns.append(col)\n",
        "    print(f'DROPPED <Column: {col}> NULL count: {null_count*100:.2f}%')\n",
        "  else:\n",
        "    print(f'<Column: {col}> NULL count: {null_count*100:.2f}%')\n",
        "  \n",
        "df = df.drop(columns=unuseful_columns)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMUqnCTP1ExS",
        "outputId": "f33ab5da-3da3-4b40-c926-bb00e99d914d"
      },
      "outputs": [],
      "source": [
        "for feature in df.columns:\n",
        "    print(f'Feature: {feature} -- {list(df[feature].unique())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows_mappings = {\n",
        "    'Protected Category': {\n",
        "        'Article 18': 'Yes',\n",
        "        'Article 1': 'Yes'\n",
        "    }\n",
        "}\n",
        "for col, mapping in rows_mappings.items():\n",
        "    df[col] = df[col].replace(mapping)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove rows with 'Candidate State' as 'First contact' or 'Imported'\n",
        "# These statuses are not relevant for the analysis\n",
        "rows_to_remove = {\n",
        "    'Candidate State': ['First contact', 'Imported']\n",
        "}\n",
        "\n",
        "for col, remove_list in rows_to_remove.items():\n",
        "    df = df[~df[col].isin(remove_list)]\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmg_n5ol1ExS"
      },
      "source": [
        "### Handle the NANs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGr8pjDc1ExT",
        "outputId": "50b0cb55-a0a6-474b-802c-6e000aced0ad"
      },
      "outputs": [],
      "source": [
        "print(f'Columns that contain NaN values:\\n {df.columns[df.isnull().any()].tolist()}')\n",
        "\n",
        "for col in df.columns[df.isnull().any()].tolist():\n",
        "  print(f'{col} values: {df[col].unique()} \\n') # Analyze each NaN containing feature first to determine the default fill value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xUD3IuZ1ExT"
      },
      "outputs": [],
      "source": [
        "fill_values = {\n",
        "    'Citizenship': 'Not Specified',\n",
        "    'Protected category': 'Not a protected category',\n",
        "    'Study area': 'Not Specified',\n",
        "    'Sector': 'Unemployed',\n",
        "    'Job Family Hiring': 'Not Specified',\n",
        "    'Job Title Hiring': 'Not Specified',\n",
        "    'vent_feedback': 'Not Specified',\n",
        "    'verall': 'Not Specified',\n",
        "    'Minimum Ral': 'Not Specified',\n",
        "    'Ral Maximum': 'Not Specified',\n",
        "    'Study Level': 'Not Specified',\n",
        "    'Current Ral': 'Not Specified',\n",
        "    'Expected Ral': 'Not Specified',\n",
        "    'Technical Skills': df['Technical Skills'].mean(),\n",
        "    'Comunication': df['Comunication'].mean(),\n",
        "    'Maturity': df['Maturity'].mean(),\n",
        "    'Dynamism': df['Dynamism'].mean(),\n",
        "    'Mobility': df['Mobility'].mean(),\n",
        "    'English': df['English'].mean()\n",
        "}\n",
        "df = df.fillna(fill_values)\n",
        "print(f'There are {df.isnull().sum().sum()} NANs')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Features Reformatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df['Residence'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rename_keywords = [\n",
        "    'ETHIOPIA',\n",
        "    'SOUTH AFRICAN REPUBLIC',\n",
        "    'USSR',\n",
        "    'YUGOSLAVIA'\n",
        "]\n",
        "\n",
        "for kw in rename_keywords:\n",
        "  mask = df['Residence'].str.contains(kw, na=False)\n",
        "  df.loc[mask, 'Residence'] = f\"{kw} » (STATE) ~ (FOREIGN)\"\n",
        "  print(f\"Formatted '{kw}' as '{kw} » (STATE) ~ (FOREIGN)'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "residence_list = df['Residence'].unique()\n",
        "state_list = [s for s in residence_list \n",
        "              if ('(STATE)' in s) \n",
        "              or ('(COUNTRY)' in s)\n",
        "              and not ('Not Specified' in s)\n",
        "              ]\n",
        "italy_list = [s for s in residence_list \n",
        "              if not ('(STATE)' in s) \n",
        "              and not ('(COUNTRY)' in s)\n",
        "              and not ('Not Specified' in s)\n",
        "              ]\n",
        "state_list = sorted({s.split(' » ')[0] for s in state_list if ' » ' in s})\n",
        "city_list = sorted({s.split(' » ')[0] for s in italy_list if ' » ' in s})\n",
        "province_list = sorted({s.split(' » ')[1].split(' ~ ')[0] for s in italy_list if ' » ' in s and ' ~ ' in s})\n",
        "region_list = sorted({s.split(' ~ ')[-1] for s in italy_list if ' ~ ' in s})\n",
        "\n",
        "print(f\"List of residence italian citys of the candidates in the dataset:\\n {city_list}\")\n",
        "print(f\"List of residence italian provinces of the candidates in the dataset:\\n {province_list}\")\n",
        "print(f\"List of residence italian regions of the candidates in the dataset:\\n {region_list}\")\n",
        "print(f\"List of residence states of the candidates in the dataset:\\n {state_list}\")\n",
        "\n",
        "def map_residence_city(value):\n",
        "    for city in city_list:\n",
        "        if city in value:\n",
        "            return city\n",
        "    return 'Not Specified'\n",
        "\n",
        "def map_residence_province(value):\n",
        "    for prov in province_list:\n",
        "        if prov in value:\n",
        "            return prov\n",
        "    return 'Not Specified'\n",
        "\n",
        "def map_residence_region(value):\n",
        "    for region in region_list:\n",
        "        if region in value:\n",
        "            return region\n",
        "    return 'Not Specified'\n",
        "\n",
        "def map_residence_state(value):\n",
        "    for state in state_list:\n",
        "        if state in value:\n",
        "            return state\n",
        "    return 'ITALY'\n",
        "\n",
        "# Applicazione sulle colonne\n",
        "df['Residence City']     = df['Residence'].apply(map_residence_city)\n",
        "df['Residence Province'] = df['Residence'].apply(map_residence_province)\n",
        "df['Residence Region']   = df['Residence'].apply(map_residence_region)\n",
        "df['Residence State']    = df['Residence'].apply(map_residence_state)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "european_countries = [\n",
        "    'ALBANIA', 'AUSTRIA', 'BELARUS', 'BELGIUM', 'BULGARIA', 'CROATIA', 'CZECH REPUBLIC',\n",
        "    'FRANCE', 'GERMANY', 'GREAT BRITAIN-NORTHERN IRELAND', 'GREECE', 'ITALY', 'LATVIA',\n",
        "    'LITHUANIA', 'LUXEMBOURG', 'MALTA', 'MOLDOVA', 'MONACO', 'MONTENEGRO', 'NETHERLANDS',\n",
        "    'NORWAY', 'POLAND', 'PORTUGAL', 'ROMANIA', 'RUSSIA', 'SAN MARINO', 'SERBIA', 'SLOVAKIA',\n",
        "    'SLOVENIA', 'SPAIN', 'SWEDEN', 'SWITZERLAND', 'UKRAINE'\n",
        "]\n",
        "df['European Residence'] = df['Residence State'].apply(lambda x: 'Yes' if x in european_countries else 'No')\n",
        "df['Italian Residence'] = df['Residence State'].apply(lambda x: 'Yes' if 'ITALY' in x else 'No')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohSRbTbT1ExU"
      },
      "source": [
        "### Features Remapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_mapping = {\n",
        "    'Study area' : {\n",
        "        'Automation/Mechatronics Engineering': 'Engineering',\n",
        "        'computer engineering': 'Engineering',\n",
        "        'chemical engineering': 'Engineering',\n",
        "        'Legal': 'Law',\n",
        "        'Mechanical engineering': 'Engineering',\n",
        "        'Telecommunications Engineering': 'Engineering',\n",
        "        'Economic - Statistics': 'Economic',\n",
        "        'Psychology': 'Scientific Field',\n",
        "        'Materials Science and Engineering': 'Engineering',\n",
        "        'Other scientific subjects': 'Scientific Field',\n",
        "        'Biomedical Engineering': 'Engineering',\n",
        "        'electronic Engineering': 'Engineering',\n",
        "        'Information Engineering': 'Engineering',\n",
        "        'Aeronautical/Aerospace/Astronautics Engineering': 'Engineering',\n",
        "        'Energy and Nuclear Engineering': 'Engineering',\n",
        "        'Informatics': 'Informatics',\n",
        "        'Management Engineering': 'Engineering',\n",
        "        'Automotive Engineering': 'Engineering',\n",
        "        'industrial engineering': 'Engineering',\n",
        "        'Other': 'Other',\n",
        "        'Surveyor': 'NO COLLEGE',\n",
        "        'Civil/Civil and Environmental Engineering': 'Engineering',\n",
        "        'Electrical Engineering': 'Engineering',\n",
        "        'Scientific maturity': 'NO COLLEGE',\n",
        "        'Chemist - Pharmaceutical': 'Medical Field',\n",
        "        'Political-Social': 'Other Humanities Subjects',\n",
        "        'Other humanities subjects': 'Other Humanities Subjects',\n",
        "        'Geo-Biological': 'Scientific Field',\n",
        "        'Linguistics': 'Linguistics',\n",
        "        'Agriculture and veterinary': 'Scientific Field',\n",
        "        'Literary': 'Other Humanities Subjects',\n",
        "        'Humanistic high school diploma': 'NO COLLEGE',\n",
        "        'Accounting': 'NO COLLEGE',\n",
        "        'Communication Sciences': 'Other Humanities Subjects',\n",
        "        'Safety Engineering': 'Engineering',\n",
        "        'Architecture': 'Scientific Field',\n",
        "        'Mathematics': 'Scientific Field',\n",
        "        'construction Engineering': 'Engineering',\n",
        "        'Petroleum Engineering': 'Engineering',\n",
        "        'Naval Engineering': 'Engineering',\n",
        "        'Artistic': 'NO COLLEGE',\n",
        "        'Not Specified': 'Other',\n",
        "        'Mathematical-physical modeling for engineering': 'Engineering',\n",
        "        'Engineering for the environment and the territory': 'Engineering',\n",
        "        'Medical': 'Medical Field',\n",
        "        'Defense and Security': 'Other',\n",
        "        'Physical education': 'Other',\n",
        "        'Statistics': 'Scientific Field',\n",
        "        'Educational/training sciences': 'Other Humanities Subjects'\n",
        "    },\n",
        "    'Age Range' : {\n",
        "        '< 20 years': '< 20 years',\n",
        "        '20 - 25 years': '20 - 25 years',\n",
        "        '26 - 30 years': '26 - 30 years',\n",
        "        '31 - 35 years': '31 - 35 years',\n",
        "        '36 - 40 years': '36 - 40 years',\n",
        "        '40 - 45 years': '40 - 45 years',\n",
        "        '> 45 years': '> 45 years'\n",
        "    }\n",
        "}\n",
        "for col, mapping in feature_mapping.items():\n",
        "    df[col] = df[col].replace(mapping)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlOnBSVF1ExV"
      },
      "source": [
        "### Target Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRjVe10r1ExV",
        "outputId": "88790b0c-536a-41d4-bbb2-1341787a8e9d"
      },
      "outputs": [],
      "source": [
        "status_positive_conditions = {\n",
        "    'Candidate State': ['Hired', 'Economic proposal', 'QM'],\n",
        "    'event_feedback': ['OK (live)', 'OK (waiting for departure)', 'OK (hired)']\n",
        "}\n",
        "\n",
        "mask = np.zeros(len(df), dtype=bool)\n",
        "for col, valid_values in status_positive_conditions.items():\n",
        "    mask |= df[col].isin(valid_values)\n",
        "\n",
        "df['Status'] = np.where(mask, 'Positive', 'Negative')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GkzsY6s1ExW"
      },
      "source": [
        "### Categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOBlkj5x1ExW"
      },
      "outputs": [],
      "source": [
        "encoding_mappings = {}\n",
        "\n",
        "categorical_columns = [\n",
        "    'Status', 'Age Range', 'Citizenship', 'Sex',\n",
        "    'Protected category', 'Study area', 'Study Title',\n",
        "    'Years Experience', 'Sector', 'Job Family Hiring',\n",
        "    'Job Title Hiring', 'Overall',\n",
        "    'Minimum Ral', 'Ral Maximum', 'Study Level',\n",
        "    'Current Ral', 'Expected Ral'\n",
        "]\n",
        "\n",
        "custom_orders = {\n",
        "    'Status': ['Negative', 'Positive'],\n",
        "    'Age Range': ['< 20 years', '20 - 25 years', '26 - 30 years',\n",
        "                  '31 - 35 years', '36 - 40 years', '40 - 45 years', '> 45 years'],\n",
        "    'Years Experience': ['[0]', '[0-1]', '[1-3]', '[3-5]', '[5-7]', '[7-10]', '[+10]'],\n",
        "}\n",
        "\n",
        "for col in categorical_columns:\n",
        "    if col in custom_orders:\n",
        "        df[f\"{col}_encoded\"] = pd.Categorical(df[col], categories=custom_orders[col], ordered=True).codes\n",
        "        encoding_mappings[col] = {cat: i for i, cat in enumerate(custom_orders[col])}\n",
        "    else:\n",
        "        encoder = LabelEncoder()\n",
        "        df[f\"{col}_encoded\"] = encoder.fit_transform(df[col].astype(str))\n",
        "        encoding_mappings[col] = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
        "\n",
        "df = df.drop(columns=categorical_columns)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIYO8Uhi1ExW"
      },
      "source": [
        "### Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lookups = ['Citizenship', 'Age Range', 'Sex', 'Protected category', 'Study area', 'Study Title', 'Years Experience']\n",
        "for lookup in lookups:\n",
        "    mapping = {v: k for k, v in encoding_mappings[lookup].items()}\n",
        "\n",
        "    distrib = Counter(df[f\"{lookup}_encoded\"])\n",
        "    distrib_df = pd.DataFrame(distrib.items(), columns=[f\"{lookup}_encoded\", 'Count'])\n",
        "    distrib_df[lookup] = distrib_df[f\"{lookup}_encoded\"].map(mapping)\n",
        "    distrib_df.head(20).plot(x=lookup, y='Count', kind='bar', legend=False)\n",
        "    plt.title(lookup)\n",
        "    plt.xticks(rotation=45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0, linewidths=.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Percentage of Hired inside each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sensitive = ['Sex_encoded', 'Age Range_encoded', 'Citizenship_encoded', 'Protected Category_encoded']\n",
        "for feature in sensitive:\n",
        "    percentage = df.groupby(feature)['STATUS'].mean().mul(100).round(2)\n",
        "    for category, perc in percentage.items():\n",
        "        print(f\"Percentage of elements where {feature} is {category} and STATUS is HIRED: {percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chi-squared Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tabelle di contingenza\n",
        "contingency_sex    = pd.crosstab(df['Sex'], df['Status'])\n",
        "contingency_age    = pd.crosstab(df['Age Range'], df['Status'])\n",
        "contingency_region = pd.crosstab(df['Residence Region'], df['Status'])\n",
        "\n",
        "# Chi-squared tests\n",
        "tables = {\n",
        "    'Sex': contingency_sex,\n",
        "    'Age Range': contingency_age,\n",
        "    'Residence Region': contingency_region\n",
        "}\n",
        "for var, table in tables.items():\n",
        "    chi2, p, dof, expected = chi2_contingency(table, correction=False)\n",
        "    test_name = 'Chi-squared'\n",
        "    \n",
        "    # se 2×2 e attese <5 → Fisher’s exact\n",
        "    if table.shape == (2,2) and (expected < 5).any():\n",
        "        _, p = fisher_exact(table)\n",
        "        test_name = \"Fisher's exact\"\n",
        "    \n",
        "    n = table.values.sum()\n",
        "    k = min(table.shape)\n",
        "    cramer_v = np.sqrt(chi2 / (n * (k-1)))\n",
        "    \n",
        "    # Stampa a video\n",
        "    print(f\"--- {var} ---\")\n",
        "    print(\"Expected frequencies:\")\n",
        "    print(pd.DataFrame(expected, index=table.index, columns=table.columns))\n",
        "    print()\n",
        "    print(f\"{test_name}: χ² = {chi2:.2f}, p = {p:.3f}, dof = {dof}, Cramér’s V = {cramer_v:.3f}\")\n",
        "    print(\"Conclusion: Significant association between two variables (Dependent)\" if p < 0.05 else \"Conclusion: No significant association between two variables (Independent)\")\n",
        "    print()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selection_order = [\n",
        "    'Imported', 'In selection', 'First contact',\n",
        "    'QM', 'Vivier', 'Economic proposal', 'Hired'\n",
        "]\n",
        "lookouts = ['Sex', 'Age Range', 'Residence Region']\n",
        "contingency_tables = {}\n",
        "\n",
        "for lookout in lookouts:\n",
        "    for i, state in enumerate(selection_order):\n",
        "        post_states = selection_order[i+1:]\n",
        "        if post_states:\n",
        "            df_state      = df[df['Candidate State'] == state]\n",
        "            df_post_state = df[df['Candidate State'].isin(post_states)]\n",
        "            contingency_tables[state] = pd.DataFrame({\n",
        "                f'Post {state}': df_post_state.groupby(lookout, observed=True).size(), state: df_state.groupby(lookout, observed=True).size()\n",
        "            }).fillna(0).astype(int)\n",
        "\n",
        "    for var, table in contingency_tables.items():\n",
        "        chi2, p, dof, expected = chi2_contingency(table, correction=False)\n",
        "        test_name = 'Chi-squared'\n",
        "        \n",
        "        # se 2×2 e attese <5 → Fisher’s exact\n",
        "        if table.shape == (2,2) and (expected < 5).any():\n",
        "            _, p = fisher_exact(table)\n",
        "            test_name = \"Fisher's exact\"\n",
        "        \n",
        "        n = table.values.sum()\n",
        "        k = min(table.shape)\n",
        "        cramer_v = np.sqrt(chi2 / (n * (k-1)))\n",
        "        \n",
        "        # Stampa a video\n",
        "        print(f\"--- {var} ---\")\n",
        "        print(f\"Contingency table (by {lookout}):\")\n",
        "        print(table)\n",
        "        print()\n",
        "        print(\"Expected frequencies:\")\n",
        "        print(pd.DataFrame(expected, index=table.index, columns=table.columns))\n",
        "        print()\n",
        "        print(f\"{test_name}: χ² = {chi2:.2f}, p = {p:.3f}, dof = {dof}, Cramér’s V = {cramer_v:.3f}\")\n",
        "        print(\"Conclusion: Significant association between two variables (Dependent)\" if p < 0.05 else \"Conclusion: No significant association between two variables (Independent)\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM7TxgLU1ExW"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PH7esVa1ExW"
      },
      "outputs": [],
      "source": [
        "df = shuffle(df, random_state=random_seed)\n",
        "X_full = df.drop(columns=['STATUS'])\n",
        "y = df['STATUS']\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y, test_size=0.2, random_state=random_seed, stratify=y)\n",
        "\n",
        "X = df.drop(columns=['STATUS', 'sex'])\n",
        "y = df['STATUS']\n",
        "s = df['sex']\n",
        "X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(X, y, s, test_size=0.2, random_state=random_seed, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = X_train.copy()\n",
        "train_df['target'] = y_train.values\n",
        "train_df['sex'] = s_train.values\n",
        "\n",
        "test_df = X_test.copy()\n",
        "test_df['target'] = y_test.values\n",
        "test_df['sex'] = s_test.values\n",
        "\n",
        "train_ds = StandardDataset(\n",
        "    train_df,\n",
        "    label_name='target',\n",
        "    favorable_classes=[1],\n",
        "    protected_attribute_names=['sex'],\n",
        "    privileged_classes=[[1]]\n",
        ")\n",
        "\n",
        "test_ds = StandardDataset(\n",
        "    test_df,\n",
        "    label_name='target',\n",
        "    favorable_classes=[1],\n",
        "    protected_attribute_names=['sex'],\n",
        "    privileged_classes=[[1]]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lfr = LFR(unprivileged_groups=[{'sex': 0}], privileged_groups=[{'sex': 1}], k=10, Ax=5, Ay=5, Az=10, max_iter=50, verbose=1)\n",
        "\n",
        "lfr = lfr.fit(train_ds)\n",
        "X_train_lfr = lfr.transform(train_ds)\n",
        "X_train_lfr_df = pd.DataFrame(X_train_lfr.features, columns=train_ds.feature_names)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(solver='liblinear')\n",
        "clf.fit(X_train_lfr_df, y_train)\n",
        "preds_lfr = clf.predict(pd.DataFrame(lfr.transform(test_ds).features, columns=train_ds.feature_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cr = CorrelationRemover(sensitive_feature_ids=[train_ds.feature_names.index('sex')])\n",
        "\n",
        "X_train_cr = cr.fit_transform(X_train)\n",
        "X_train_cr_df = pd.DataFrame(X_train_cr.features, columns=train_ds.feature_names)\n",
        "\n",
        "X_test_cr  = cr.transform(X_test)\n",
        "X_test_cr_df = pd.DataFrame(X_test_cr.features, columns=train_ds.feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gfc = GerryFairClassifier(\n",
        "    unprivileged_groups=[{'sex': 0}],\n",
        "    privileged_groups=[{'sex': 1}],\n",
        "    estimator=LogisticRegression(solver='liblinear'),\n",
        "    constraints='demographic_parity'\n",
        ")\n",
        "gfc.fit(train_ds)\n",
        "pred_gfc = gfc.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pr = PrejudiceRemover(\n",
        "    sensitive_attr='sex',\n",
        "    eta=25.0\n",
        ")\n",
        "pr.fit(train_ds)\n",
        "pred_pr = pr.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mfc = MetaFairClassifier(\n",
        "    unprivileged_groups=[{'sex': 0}],\n",
        "    privileged_groups=[{'sex': 1}],\n",
        "    sensitive_attr='sex',\n",
        "    tau=0.8\n",
        ")\n",
        "mfc.fit(train_ds)\n",
        "pred_mfc = mfc.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eop = EqOddsPostprocessing(\n",
        "    unprivileged_groups=[{'sex': 0}],\n",
        "    privileged_groups=[{'sex': 1}]\n",
        ")\n",
        "eop = eop.fit(train_ds, gfc.predict(train_ds))\n",
        "pred_eop = eop.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roc = RejectOptionClassification(\n",
        "    unprivileged_groups=[{'sex': 0}],\n",
        "    privileged_groups=[{'sex': 1}],\n",
        "    low_class_thresh=0.01,\n",
        "    high_class_thresh=0.99,\n",
        "    num_class_thresh=100,\n",
        "    metric_name='Average odds difference'\n",
        ")\n",
        "roc = roc.fit(train_ds, gfc.predict(train_ds))\n",
        "pred_roc = roc.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_fairness_metrics(y_true, y_pred, sensitive_features, label=None):\n",
        "    mf = MetricFrame(\n",
        "        metrics={\n",
        "            'selection_rate': selection_rate,\n",
        "            'dp_diff': demographic_parity_difference,\n",
        "            'eo_diff': equalized_odds_difference,\n",
        "            'fpr': false_positive_rate,\n",
        "            'fnr': false_negative_rate,\n",
        "            'count': count\n",
        "        },\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        sensitive_features=sensitive_features\n",
        "    )\n",
        "    if label:\n",
        "        print(f\"=== {label} ===\")\n",
        "    print(mf.by_group)\n",
        "    print(\"Overall:\", mf.overall, \"\\n\")\n",
        "    return mf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-Processing\n",
        "compute_fairness_metrics(y_test, preds_lfr, s_test, label=\"LFR + LogisticRegression\")\n",
        "\n",
        "# In-processing\n",
        "compute_fairness_metrics(y_test, pred_gfc.ravel(), s_test, label=\"GerryFairClassifier\")\n",
        "compute_fairness_metrics(y_test, pred_pr.ravel(), s_test, label=\"PrejudiceRemover\")\n",
        "compute_fairness_metrics(y_test, pred_mfc.ravel(), s_test, label=\"MetaFairClassifier\")\n",
        "\n",
        "# Post-processing\n",
        "compute_fairness_metrics(y_test, pred_eop.ravel(), s_test, label=\"EqOddsPostprocessing\")\n",
        "compute_fairness_metrics(y_test, pred_roc.ravel(), s_test, label=\"RejectOptionClassification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBN9yJ1ExW"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7neYICfm1ExW"
      },
      "outputs": [],
      "source": [
        "def create_model(seed):\n",
        "    tf.random.set_seed(seed)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=22, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Neural Network': create_model(random_seed)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "bSyQkZB91ExX",
        "outputId": "166c5665-77b9-4e4b-d2b7-e20eef41a9d7"
      },
      "outputs": [],
      "source": [
        "metrics = []\n",
        "predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name in ['Linear Regression', 'Decision Tree', 'Naive Bayes', 'XGBoost', 'KNN']:\n",
        "        model.fit(X_train_full, y_train_full)\n",
        "    elif name in ['Neural Network']:\n",
        "        model.fit(X_train_full, y_train_full, epochs=15, batch_size=64, validation_split=0.2)\n",
        "    else:\n",
        "        print(\"Error in Models!\"); break\n",
        "\n",
        "    y_pred = model.predict(X_test_full)\n",
        "\n",
        "    if name in ['Linear Regression', 'XGBoost', 'Neural Network']:\n",
        "        y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    accuracy = round(accuracy_score(y_test_full, y_pred), 3)\n",
        "    precision = round(precision_score(y_test_full, y_pred), 3)\n",
        "    recall = round(recall_score(y_test_full, y_pred), 3)\n",
        "    f1 = round(f1_score(y_test_full, y_pred), 3)\n",
        "    roc_auc = round(roc_auc_score(y_test_full, y_pred), 3)\n",
        "\n",
        "    metrics.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1,\n",
        "        'ROC AUC': roc_auc\n",
        "    })\n",
        "    predictions[name] = y_pred\n",
        "\n",
        "metrics = pd.DataFrame(metrics)\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Linear Regression' : predictions['Linear Regression'],\n",
        "    'Decision Tree' : predictions['Decision Tree'],\n",
        "    'Naive Bayes' : predictions['Naive Bayes'],\n",
        "    'XGBoost' : predictions['XGBoost'],\n",
        "    'kNN' : predictions['KNN'],\n",
        "    'Neural Network' : predictions['Neural Network']\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GybD2fNN1Exa"
      },
      "source": [
        "## Fairness Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ws-DQHm1Exa"
      },
      "source": [
        "#### **3.1 Demographic Parity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCdj5tff1Exb"
      },
      "outputs": [],
      "source": [
        "sensitive_features = [' Sex_encoded', ' Age Range_encoded', ' Citizenship_encoded', ' Protected category_encoded']\n",
        "non_sensitive_features = ['Technical Skills', 'Comunication', 'Maturity', 'Dynamism', 'Mobility',\n",
        "       'English', ' Study area_encoded', ' Study Title_encoded', ' Years Experience_encoded', ' Sector_encoded', ' Job Family Hiring_encoded',\n",
        "       ' Job Title Hiring_encoded', ' Overall_encoded', ' Years Experience.1_encoded',' Minimum Ral_encoded', ' Ral Maximum_encoded',\n",
        "       ' Study Level_encoded', 'Current Ral_encoded', 'Expected Ral_encoded']\n",
        "models_list = [model for model in models]\n",
        "tolerance = 0.15\n",
        "significance_level = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzubVOQ61Exb"
      },
      "outputs": [],
      "source": [
        "def calculate_demographic_parity(predictions, sensitive_attribute, name, significance_level, tolerance, activate_check=False):\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'predictions': predictions,\n",
        "        'sensitive_attribute': sensitive_attribute\n",
        "    })\n",
        "    prop = df.groupby('sensitive_attribute')['predictions'].mean()\n",
        "    \n",
        "    if activate_check:\n",
        "        print(f\"===\\n{name}\\n{prop}\")\n",
        "\n",
        "    if prop.shape[0] == 2:\n",
        "        return 'T' if (prop.max() - prop.min()) <= tolerance else False\n",
        "    else:\n",
        "        contingency_table = pd.crosstab(df['predictions'], df['sensitive_attribute'])\n",
        "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "        if activate_check and (expected < 5).any():\n",
        "            print(f\"Sparse contingency for {name}\")\n",
        "                \n",
        "        return 'T' if p > significance_level else False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "dYqVcPet1Exb",
        "outputId": "6422f8a3-7d22-4ab2-d8b8-867596fc79dc"
      },
      "outputs": [],
      "source": [
        "table = []\n",
        "for model in models:\n",
        "    row = []\n",
        "    for sensitive_feature in sensitive_features:\n",
        "        result = calculate_demographic_parity(predictions[model], X_test_full[sensitive_feature], sensitive_feature, significance_level, tolerance, activate_check=True)\n",
        "        row.append(result)\n",
        "    table.append(row)\n",
        "sf_df = pd.DataFrame(table, index = models_list, columns=sensitive_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ1zcOEM1Exb"
      },
      "source": [
        "#### **3.2 Equalized odds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXbOhlKv1Exb"
      },
      "outputs": [],
      "source": [
        "def calculate_equalized_odds(predictions, true_labels, sensitive_attribute, name, tolerance, activate_check=False):\n",
        "    df = pd.DataFrame({\n",
        "        'predictions': predictions,\n",
        "        'true_labels': true_labels,\n",
        "        'sensitive_attribute': sensitive_attribute\n",
        "    })\n",
        "    tprs, fprs = [], []\n",
        "    for _, group_df in df.groupby('sens'):\n",
        "        tn, fp, fn, tp = confusion_matrix(group_df['true_labels'], group_df['predictions'], labels=[0, 1]).ravel()\n",
        "        tprs.append(tp / (tp + fn) if tp + fn != 0 else 0)\n",
        "        fprs.append(fp / (fp + tn) if fp + tn != 0 else 0)\n",
        "\n",
        "    max_tpr_diff = max(tprs) - min(tprs)\n",
        "    max_fpr_diff = max(fprs) - min(fprs)\n",
        "\n",
        "    if activate_check:\n",
        "            print(f\"===\\n{name}\\nMax FPR diff: {max_fpr_diff}\\nMax TPR diff: {max_tpr_diff}\")\n",
        "\n",
        "    return 'T' if (max_tpr_diff <= 2 * tolerance and max_fpr_diff <= 2 * tolerance) else False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "9KSoLzk21Exb",
        "outputId": "6f420612-90e8-4537-f6fb-43add15ddac2"
      },
      "outputs": [],
      "source": [
        "table = []\n",
        "for model in models:\n",
        "    row = []\n",
        "    for sensitive_feature in sensitive_features:\n",
        "        result = calculate_equalized_odds(predictions[model], y_test_full, X_test_full[sensitive_feature], sensitive_feature, tolerance, activate_check=False)\n",
        "        row.append(result)\n",
        "    table.append(row)\n",
        "sf_df = pd.DataFrame(table, index = models_list, columns=sensitive_features)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
