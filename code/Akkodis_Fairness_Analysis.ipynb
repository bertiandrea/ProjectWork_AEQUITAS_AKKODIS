{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "%pip install pandas matplotlib seaborn scikit-learn openpyxl tensorflow xgboost aif360\n",
        "%pip install \"aif360[Reductions, inFairness]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuNMyoGU1ExN",
        "outputId": "d5147e88-16d9-4b30-8bc3-48067a5f22d7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from scipy.stats import chi2_contingency, fisher_exact\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LinearRegression,                 LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential # type: ignore\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization # type: ignore\n",
        "\n",
        "from aif360.datasets import BinaryLabelDataset, StandardDataset\n",
        "from aif360.algorithms.preprocessing import LFR\n",
        "from fairlearn.preprocessing import CorrelationRemover\n",
        "from aif360.algorithms.inprocessing import GerryFairClassifier, PrejudiceRemover, MetaFairClassifier\n",
        "from aif360.algorithms.postprocessing import EqOddsPostprocessing, RejectOptionClassification\n",
        "\n",
        "from fairlearn.metrics import MetricFrame\n",
        "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference, selection_rate, false_positive_rate, false_negative_rate, count\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "random_seed = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH = 'C:/Users/andre/Desktop/ProjectWork_AEQUITAS_AKKODIS/'\n",
        "df = (\n",
        "    pd.read_excel(PATH + 'data/Dataset_2.0_Akkodis.xlsx')\n",
        "      .rename(columns=lambda c: c.lstrip().title())\n",
        ")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {}\n",
        "config['drop_columns'] = [\n",
        "    'Id', 'Tag', 'Year Of Insertion',\n",
        "    'Year Of Recruitment', 'Recruitment Request', 'Assumption Headquarters',\n",
        "    'Event_Type__Val', 'Linked_Search__Key', 'Job Description', \n",
        "    'Candidate Profile', 'Akkodis Headquarters', 'Standing/Position', 'Last Role', 'Study Area.1',\n",
        "    'Years Experience.1']\n",
        "config['drop_rows'] = {\n",
        "    'Candidate State': ['First contact', 'Imported']\n",
        "}\n",
        "config['remap_rows'] = {\n",
        "    'Protected Category': {\n",
        "        'Article 18': 'Yes',\n",
        "        'Article 1': 'Yes'\n",
        "    },\n",
        "    'Residence': {\n",
        "        'ETHIOPIA': 'ETHIOPIA » (STATE) ~ (FOREIGN)',\n",
        "        'SOUTH AFRICAN REPUBLIC': 'SOUTH AFRICAN REPUBLIC » (STATE) ~ (FOREIGN)',\n",
        "        'USSR': 'USSR » (STATE) ~ (FOREIGN)',\n",
        "        'YUGOSLAVIA': 'YUGOSLAVIA » (STATE) ~ (FOREIGN)'\n",
        "    },\n",
        "    'Study Area' : {\n",
        "        'Automation/Mechatronics Engineering': 'Engineering',\n",
        "        'computer engineering': 'Engineering',\n",
        "        'chemical engineering': 'Engineering',\n",
        "        'Legal': 'Law',\n",
        "        'Mechanical engineering': 'Engineering',\n",
        "        'Telecommunications Engineering': 'Engineering',\n",
        "        'Economic - Statistics': 'Economic',\n",
        "        'Psychology': 'Scientific Field',\n",
        "        'Materials Science and Engineering': 'Engineering',\n",
        "        'Other scientific subjects': 'Scientific Field',\n",
        "        'Biomedical Engineering': 'Engineering',\n",
        "        'electronic Engineering': 'Engineering',\n",
        "        'Information Engineering': 'Engineering',\n",
        "        'Aeronautical/Aerospace/Astronautics Engineering': 'Engineering',\n",
        "        'Energy and Nuclear Engineering': 'Engineering',\n",
        "        'Informatics': 'Informatics',\n",
        "        'Management Engineering': 'Engineering',\n",
        "        'Automotive Engineering': 'Engineering',\n",
        "        'industrial engineering': 'Engineering',\n",
        "        'Other': 'Other',\n",
        "        'Surveyor': 'NO COLLEGE',\n",
        "        'Civil/Civil and Environmental Engineering': 'Engineering',\n",
        "        'Electrical Engineering': 'Engineering',\n",
        "        'Scientific maturity': 'NO COLLEGE',\n",
        "        'Chemist - Pharmaceutical': 'Medical Field',\n",
        "        'Political-Social': 'Other Humanities Subjects',\n",
        "        'Other humanities subjects': 'Other Humanities Subjects',\n",
        "        'Geo-Biological': 'Scientific Field',\n",
        "        'Linguistics': 'Linguistics',\n",
        "        'Agriculture and veterinary': 'Scientific Field',\n",
        "        'Literary': 'Other Humanities Subjects',\n",
        "        'Humanistic high school diploma': 'NO COLLEGE',\n",
        "        'Accounting': 'NO COLLEGE',\n",
        "        'Communication Sciences': 'Other Humanities Subjects',\n",
        "        'Safety Engineering': 'Engineering',\n",
        "        'Architecture': 'Scientific Field',\n",
        "        'Mathematics': 'Scientific Field',\n",
        "        'construction Engineering': 'Engineering',\n",
        "        'Petroleum Engineering': 'Engineering',\n",
        "        'Naval Engineering': 'Engineering',\n",
        "        'Artistic': 'NO COLLEGE',\n",
        "        'Not Specified': 'Other',\n",
        "        'Mathematical-physical modeling for engineering': 'Engineering',\n",
        "        'Engineering for the environment and the territory': 'Engineering',\n",
        "        'Medical': 'Medical Field',\n",
        "        'Defense and Security': 'Other',\n",
        "        'Physical education': 'Other',\n",
        "        'Statistics': 'Scientific Field',\n",
        "        'Educational/training sciences': 'Other Humanities Subjects'\n",
        "    }\n",
        "}\n",
        "config['fill_nan_columns'] = {\n",
        "    'Citizenship': 'Not Specified',\n",
        "    'Protected Category': 'Not a protected category',\n",
        "    'Study area': 'Not Specified',\n",
        "    'Sector': 'Unemployed',\n",
        "    'Job Family Hiring': 'Not Specified',\n",
        "    'Job Title Hiring': 'Not Specified',\n",
        "    'Event_Feedback': 'Not Specified',\n",
        "    'Overall': 'Not Specified',\n",
        "    'Minimum Ral': 'Not Specified',\n",
        "    'Ral Maximum': 'Not Specified',\n",
        "    'Study Level': 'Not Specified',\n",
        "    'Current Ral': 'Not Specified',\n",
        "    'Expected Ral': 'Not Specified',\n",
        "    'Technical Skills': '%MEAN%',\n",
        "    'Comunication': '%MEAN%',\n",
        "    'Maturity': '%MEAN%',\n",
        "    'Dynamism': '%MEAN%',\n",
        "    'Mobility': '%MEAN%',\n",
        "    'English': '%MEAN%'\n",
        "}\n",
        "config['drop_nan_columns_threshold'] = '100.0'\n",
        "config['feature_remapping'] = {\n",
        "    'Residence': {\n",
        "        'lists': {\n",
        "            'state':    {'src':'Residence', 'inc':['(STATE)','(COUNTRY)'], 'exc':['Not Specified'], 'split':(' » ', 0)                    },\n",
        "            'city':     {'src':'Residence', 'exc':['(STATE)','(COUNTRY)','Not Specified'],          'split':(' » ', 0)                    },\n",
        "            'province': {'src':'Residence', 'exc':['(STATE)','(COUNTRY)','Not Specified'],          'split':(' » ', 1), 'post':(' ~ ', 0) },\n",
        "            'region':   {'src':'Residence', 'exc':['(STATE)','(COUNTRY)','Not Specified'],          'split':(' ~ ',-1)                    },\n",
        "        },\n",
        "        'fields': {\n",
        "            'Residence City':     {'src':'Residence', 'list':'city',     'def':'Not Specified'},\n",
        "            'Residence Province': {'src':'Residence', 'list':'province', 'def':'Not Specified'},\n",
        "            'Residence Region':   {'src':'Residence', 'list':'region',   'def':'Not Specified'},\n",
        "            'Residence State':    {'src':'Residence', 'list':'state',    'def':'ITALY'},\n",
        "            'European Residence': {'src':'Residence State', 'in':'eu',    'y':'Yes','n':'No'},\n",
        "            'Italian Residence':  {'src':'Residence State', 'eq':'ITALY', 'y':'Yes','n':'No'},\n",
        "        },\n",
        "        'eu': [\n",
        "            'ALBANIA', 'AUSTRIA', 'BELARUS', 'BELGIUM', 'BULGARIA', 'CROATIA', 'CZECH REPUBLIC',\n",
        "            'FRANCE', 'GERMANY', 'GREAT BRITAIN-NORTHERN IRELAND', 'GREECE', 'ITALY', 'LATVIA',\n",
        "            'LITHUANIA', 'LUXEMBOURG', 'MALTA', 'MOLDOVA', 'MONACO', 'MONTENEGRO', 'NETHERLANDS',\n",
        "            'NORWAY', 'POLAND', 'PORTUGAL', 'ROMANIA', 'RUSSIA', 'SAN MARINO', 'SERBIA', 'SLOVAKIA',\n",
        "            'SLOVENIA', 'SPAIN', 'SWEDEN', 'SWITZERLAND', 'UKRAINE'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "config['status_positive_conditions'] = {\n",
        "    'Candidate State': ['Hired', 'Economic proposal', 'QM'],\n",
        "    'Event_Feedback': ['OK (live)', 'OK (waiting for departure)', 'OK (hired)']\n",
        "}\n",
        "config['categorical_columns'] = [\n",
        "    'Candidate State', 'Event_Feedback', \n",
        "    'Residence City', 'Residence Province', 'Residence Region', 'Residence State', 'European Residence', 'Italian Residence',\n",
        "    'Age Range', 'Sex',\n",
        "    'Protected Category', 'Study Area', 'Study Title',\n",
        "    'Years Experience', 'Sector', 'Job Family Hiring',\n",
        "    'Job Title Hiring', 'Overall',\n",
        "    'Minimum Ral', 'Ral Maximum', 'Study Level',\n",
        "    'Current Ral', 'Expected Ral'\n",
        "]\n",
        "config['categorical_columns_custom_orders'] = {\n",
        "    'Candidate State': ['Imported', 'First contact', 'In Selection', 'QM', 'Vivier', 'Economic proposal', 'Hired'],\n",
        "    'Age Range': ['< 20 years', '20 - 25 years', '26 - 30 years', '31 - 35 years', '36 - 40 years', '40 - 45 years', '> 45 years'],\n",
        "    'Years Experience': ['Not Specified', '[0]', '[0-1]', '[1-3]', '[3-5]', '[5-7]', '[7-10]', '[+10]'],\n",
        "    'Study Title': ['Middle school diploma', 'High school graduation', 'Professional qualification', 'Three-year degree', 'master\\'s degree', 'Five-year degree', 'Doctorate'],\n",
        "    'RAL': ['Not Specified', '-20k', '20-22k', '22-24k', '24-26k', '26-28k', '28-30k', '30-32k', '32-34k', '34-36k', '36-38k', '38-40k', '40-42k', '42-44k', '44-46k', '46-48k', '48-50k', '+50k']\n",
        "}\n",
        "config['visualize_columns'] = ['Age Range', 'Sex', 'Protected Category', 'Study Area', 'Study Title', 'Years Experience']\n",
        "config['sensitive_columns'] = ['Sex_encoded', 'Age Range_encoded', 'Protected Category_encoded']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nh5kHyh1ExO"
      },
      "source": [
        "### Clean Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop_duplicates(subset='Id', keep='last')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpIFJx6X1ExQ"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=config['drop_columns'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col, remove_list in config['drop_rows'].items():\n",
        "    df = df[~df[col].isin(remove_list)]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col, mapping in config['remap_rows'].items():\n",
        "    df[col] = df[col].replace(mapping)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for feature in df.columns:\n",
        "    print(f'Feature: {feature} -- {list(df[feature].unique())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmg_n5ol1ExS"
      },
      "source": [
        "### Handle the NANs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGr8pjDc1ExT",
        "outputId": "50b0cb55-a0a6-474b-802c-6e000aced0ad"
      },
      "outputs": [],
      "source": [
        "print(f'Columns that contain NaN values:\\n {df.columns[df.isnull().any()].tolist()}')\n",
        "\n",
        "for col in df.columns[df.isnull().any()].tolist():\n",
        "  print(f'{col} values: {df[col].unique()} \\n') # Analyze each NaN containing feature first to determine the default fill value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unuseful_columns = []\n",
        "for col in df.columns:\n",
        "  null_count = df[col].isna().sum() / df.shape[0]\n",
        "  if null_count > config['drop_nan_columns_threshold'].astype(float):\n",
        "    unuseful_columns.append(col)\n",
        "  \n",
        "df = df.drop(columns=unuseful_columns)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xUD3IuZ1ExT"
      },
      "outputs": [],
      "source": [
        "for col, filler in config['fill_nan_rows'].items():\n",
        "    if filler == '%MEAN%':\n",
        "        media = round(df[col].mean())\n",
        "    df[col].fillna(media, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'There are {df.isnull().sum().sum()} NANs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Features Reformatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df['Residence'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_lists(df, specs):\n",
        "    out = {}\n",
        "    for name, p in specs.items():\n",
        "        base = out.get(p['src'], df[p['src']].dropna().astype(str).unique())\n",
        "        # filtra inc/exc\n",
        "        items = [\n",
        "            x for x in base\n",
        "            if not any(exc in x for exc in p.get('exc', []))\n",
        "               and (any(inc in x for inc in p.get('inc', [])) if 'inc' in p else True)\n",
        "        ]\n",
        "        # split / post-split\n",
        "        for key in ('split', 'post'):\n",
        "            if key in p:\n",
        "                sep, idx = p[key]\n",
        "                items = [x.split(sep)[idx] if sep in x else x for x in items]\n",
        "        out[name] = sorted(set(items))\n",
        "    return out\n",
        "\n",
        "def apply_field(val, p, lists, feature_cfg):\n",
        "    # pattern‐match su lista\n",
        "    if 'list' in p:\n",
        "        return next((x for x in lists[p['list']] if x in str(val)), p['def'])\n",
        "    # appartenenza in lista\n",
        "    if 'in' in p:\n",
        "        return p['y'] if val in feature_cfg[p['in']] else p['n']\n",
        "    # confronto esatto\n",
        "    if 'eq' in p:\n",
        "        return p['y'] if str(val) == p['eq'] else p['n']\n",
        "\n",
        "# Applica per ogni feature definita ———\n",
        "for feature, feat_cfg in config['feature_remapping'].items():\n",
        "    # Genera le liste specifiche\n",
        "    lists = gen_lists(df, feat_cfg['lists'])\n",
        "    # Mappa ogni campo\n",
        "    for col_name, col_cfg in feat_cfg['fields'].items():\n",
        "        df[col_name] = df[col_cfg['src']].apply(lambda v, cfg=col_cfg: apply_field(v, cfg, lists, feat_cfg))\n",
        "    # Elimina le colonne originali\n",
        "    df = df.drop(columns=feature, errors='ignore')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlOnBSVF1ExV"
      },
      "source": [
        "### Target Column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRjVe10r1ExV",
        "outputId": "88790b0c-536a-41d4-bbb2-1341787a8e9d"
      },
      "outputs": [],
      "source": [
        "config['categorical_columns'].append('Status')\n",
        "\n",
        "mask = np.zeros(len(df), dtype=bool)\n",
        "for col, valid_values in config['status_positive_conditions'].items():\n",
        "    mask |= df[col].isin(valid_values)\n",
        "df['Status'] = np.where(mask, 'Positive', 'Negative')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GkzsY6s1ExW"
      },
      "source": [
        "### Categorical columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOBlkj5x1ExW"
      },
      "outputs": [],
      "source": [
        "encoding_mappings = {}\n",
        "\n",
        "for col in config['categorical_columns']:\n",
        "    if col in config['categorical_columns_custom_orders']:\n",
        "        df[f\"{col}_encoded\"] = pd.Categorical(df[col], categories=config['categorical_columns_custom_orders'][col], ordered=True).codes\n",
        "        encoding_mappings[col] = {cat: i for i, cat in enumerate(config['categorical_columns_custom_orders'][col])}\n",
        "    else:\n",
        "        encoder = LabelEncoder()\n",
        "        df[f\"{col}_encoded\"] = encoder.fit_transform(df[col].astype(str))\n",
        "        encoding_mappings[col] = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
        "\n",
        "df = df.drop(columns=config['categorical_columns'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for feature in df.columns:\n",
        "    print(f'Feature: {feature} -- {list(df[feature].unique())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIYO8Uhi1ExW"
      },
      "source": [
        "### Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for lookup in config['visualize_columns']:\n",
        "    mapping = {v: k for k, v in encoding_mappings[lookup].items()}\n",
        "    codes = sorted(mapping.keys())\n",
        "    distrib = Counter(df[f\"{lookup}_encoded\"])\n",
        "    print(f\"Distribution of {lookup}: {distrib}\")\n",
        "    labels = [mapping[c] for c in codes]\n",
        "    counts = [distrib[c] for c in codes]\n",
        "    distrib_df = pd.DataFrame({lookup: labels, 'Count': counts})\n",
        "    distrib_df.head(20).plot(x=lookup, y='Count', kind='bar', legend=False)\n",
        "    plt.title(lookup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 12))\n",
        "sns.heatmap(df.corr().round(2), annot=True, cmap='coolwarm', center=0, linewidths=.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Percentage of Positive Status inside each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(encoding_mappings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "status_map = {v: k for k, v in encoding_mappings['Status'].items()}\n",
        "for feature in config['sensitive_columns']:\n",
        "    cat_map = {v: k for k, v in encoding_mappings[feature.replace('_encoded', '')].items()}\n",
        "    percentage = df.groupby(feature)['Status_encoded'].mean().mul(100).round(2) # Positive\n",
        "    for cat, perc in percentage.items():\n",
        "        print(f\"Feature: {feature} - Feature Val: {cat_map[cat]} - Status Val: {status_map[1]}-> {perc:.2f}%\")\n",
        "        print(f\"Feature: {feature} - Feature Val: {cat_map[cat]} - Status Val: {status_map[0]}-> {(100 - perc):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save to File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_excel(PATH + 'data/Dataset_Preprocessed.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chi-squared Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tabelle di contingenza\n",
        "contingency_sex    = pd.crosstab(df['Sex_encoded'], df['Status_encoded'])\n",
        "contingency_age    = pd.crosstab(df['Age Range_encoded'], df['Status_encoded'])\n",
        "contingency_region = pd.crosstab(df['Residence Region_encoded'], df['Status_encoded'])\n",
        "\n",
        "# Chi-squared tests\n",
        "tables = {\n",
        "    'Sex': contingency_sex,\n",
        "    'Age Range': contingency_age,\n",
        "    'Residence Region': contingency_region\n",
        "}\n",
        "for var, table in tables.items():\n",
        "    chi2, p, dof, expected = chi2_contingency(table, correction=False)\n",
        "    test_name = 'Chi-squared'\n",
        "    \n",
        "    # se 2×2 e attese <5 → Fisher’s exact\n",
        "    if table.shape == (2,2) and (expected < 5).any():\n",
        "        _, p = fisher_exact(table)\n",
        "        test_name = \"Fisher's exact\"\n",
        "    \n",
        "    n = table.values.sum()\n",
        "    k = min(table.shape)\n",
        "    cramer_v = np.sqrt(chi2 / (n * (k-1)))\n",
        "    \n",
        "    # Stampa a video\n",
        "    print(f\"--- {var} ---\")\n",
        "    print(\"Expected frequencies:\")\n",
        "    print(pd.DataFrame(expected, index=table.index, columns=table.columns))\n",
        "    print()\n",
        "    print(f\"{test_name}: χ² = {chi2:.2f}, p = {p:.3f}, dof = {dof}, Cramér’s V = {cramer_v:.3f}\")\n",
        "    print(\"Conclusion: Significant association between two variables (Dependent)\" if p < 0.05 else \"Conclusion: No significant association between two variables (Independent)\")\n",
        "    print()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for feature in df.columns:\n",
        "    print(f'Feature: {feature} -- {list(df[feature].unique())}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selection_order = [\n",
        "    'Imported', 'In selection', 'First contact',\n",
        "    'QM', 'Vivier', 'Economic proposal', 'Hired'\n",
        "]\n",
        "lookouts = ['Sex_encoded', 'Age Range_encoded', 'Residence Region_encoded']\n",
        "\n",
        "for lookout in lookouts:\n",
        "    contingency_tables = {}\n",
        "    cat_map = {v: k for k, v in encoding_mappings[lookout.replace('_encoded', '')].items()}\n",
        "    for i, state in enumerate(selection_order):\n",
        "        if state not in encoding_mappings['Candidate State']:\n",
        "            continue\n",
        "\n",
        "        post_states = selection_order[i+1:]\n",
        "        if not post_states:\n",
        "            continue\n",
        "\n",
        "        state_code = encoding_mappings['Candidate State'][state]\n",
        "        post_state_codes = [encoding_mappings['Candidate State'][s] for s in post_states if s in encoding_mappings['Candidate State']]\n",
        "\n",
        "        df_state      = df[df['Candidate State_encoded'] == state_code]\n",
        "        df_post_state = df[df['Candidate State_encoded'].isin(post_state_codes)]\n",
        "\n",
        "        table = pd.DataFrame({\n",
        "            f'Post {state}': df_post_state.groupby(lookout, observed=True).size(),\n",
        "            state:            df_state.groupby(lookout, observed=True).size()\n",
        "        }).fillna(0).astype(int)\n",
        "        table.index = table.index.map(cat_map)\n",
        "        contingency_tables[state] = table\n",
        "\n",
        "    for var, table in contingency_tables.items():\n",
        "        if table.empty or table.values.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        chi2, p, dof, expected = chi2_contingency(table, correction=False)\n",
        "        test_name = 'Chi-squared'\n",
        "        if table.shape == (2,2) and (expected < 5).any():\n",
        "            _, p = fisher_exact(table)\n",
        "            test_name = \"Fisher's exact\"\n",
        "\n",
        "        n = table.values.sum()\n",
        "        k = min(table.shape)\n",
        "        cramer_v = np.sqrt(chi2 / (n * (k-1)))\n",
        "\n",
        "        print(f\"--- {var} (by {lookout}) ---\")\n",
        "        print(table, \"\\n\")\n",
        "        print(\"Expected frequencies:\")\n",
        "        print(pd.DataFrame(expected, index=table.index, columns=table.columns), \"\\n\")\n",
        "        print(f\"{test_name}: χ²={chi2:.2f}, p={p:.3f}, dof={dof}, Cramér’s V={cramer_v:.3f}\")\n",
        "        print(\"Dependent\" if p<0.05 else \"Independent\", \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM7TxgLU1ExW"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PH7esVa1ExW"
      },
      "outputs": [],
      "source": [
        "df = shuffle(df, random_state=random_seed)\n",
        "X_full = df.drop(columns=['STATUS'])\n",
        "y = df['STATUS']\n",
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y, test_size=0.2, random_state=random_seed, stratify=y)\n",
        "\n",
        "X = df.drop(columns=['STATUS', 'sex'])\n",
        "y = df['STATUS']\n",
        "s = df['sex']\n",
        "X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(X, y, s, test_size=0.2, random_state=random_seed, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = X_train.copy()\n",
        "train_df['target'] = y_train.values\n",
        "train_df['sex'] = s_train.values\n",
        "\n",
        "test_df = X_test.copy()\n",
        "test_df['target'] = y_test.values\n",
        "test_df['sex'] = s_test.values\n",
        "\n",
        "train_ds = StandardDataset(\n",
        "    train_df,\n",
        "    label_name='target',\n",
        "    favorable_classes=[1],\n",
        "    protected_attribute_names=['sex'],\n",
        "    privileged_classes=[[1]]\n",
        ")\n",
        "\n",
        "test_ds = StandardDataset(\n",
        "    test_df,\n",
        "    label_name='target',\n",
        "    favorable_classes=[1],\n",
        "    protected_attribute_names=['sex'],\n",
        "    privileged_classes=[[1]]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lfr = LFR(unprivileged_groups=[{'sex': 0}], privileged_groups=[{'sex': 1}], k=10, Ax=5, Ay=5, Az=10, max_iter=50, verbose=1)\n",
        "\n",
        "lfr = lfr.fit(train_ds)\n",
        "X_train_lfr = lfr.transform(train_ds)\n",
        "X_train_lfr_df = pd.DataFrame(X_train_lfr.features, columns=train_ds.feature_names)\n",
        "\n",
        "\n",
        "clf = LogisticRegression(solver='liblinear')\n",
        "clf.fit(X_train_lfr_df, y_train)\n",
        "preds_lfr = clf.predict(pd.DataFrame(lfr.transform(test_ds).features, columns=train_ds.feature_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cr = CorrelationRemover(sensitive_feature_ids=[train_ds.feature_names.index('sex')])\n",
        "\n",
        "X_train_cr = cr.fit_transform(X_train)\n",
        "X_train_cr_df = pd.DataFrame(X_train_cr.features, columns=train_ds.feature_names)\n",
        "\n",
        "X_test_cr  = cr.transform(X_test)\n",
        "X_test_cr_df = pd.DataFrame(X_test_cr.features, columns=train_ds.feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gfc = GerryFairClassifier(\n",
        "    unprivileged_groups=[{'sex': 0}],\n",
        "    privileged_groups=[{'sex': 1}],\n",
        "    estimator=LogisticRegression(solver='liblinear'),\n",
        "    constraints='demographic_parity'\n",
        ")\n",
        "gfc.fit(train_ds)\n",
        "pred_gfc = gfc.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pr = PrejudiceRemover(\n",
        "    sensitive_attr='sex',\n",
        "    eta=25.0\n",
        ")\n",
        "pr.fit(train_ds)\n",
        "pred_pr = pr.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mfc = MetaFairClassifier(\n",
        "    unprivileged_groups=[{'sex': 0}],\n",
        "    privileged_groups=[{'sex': 1}],\n",
        "    sensitive_attr='sex',\n",
        "    tau=0.8\n",
        ")\n",
        "mfc.fit(train_ds)\n",
        "pred_mfc = mfc.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eop = EqOddsPostprocessing(\n",
        "    unprivileged_groups=[{'sex': 0}],\n",
        "    privileged_groups=[{'sex': 1}]\n",
        ")\n",
        "eop = eop.fit(train_ds, gfc.predict(train_ds))\n",
        "pred_eop = eop.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roc = RejectOptionClassification(\n",
        "    unprivileged_groups=[{'sex': 0}],\n",
        "    privileged_groups=[{'sex': 1}],\n",
        "    low_class_thresh=0.01,\n",
        "    high_class_thresh=0.99,\n",
        "    num_class_thresh=100,\n",
        "    metric_name='Average odds difference'\n",
        ")\n",
        "roc = roc.fit(train_ds, gfc.predict(train_ds))\n",
        "pred_roc = roc.predict(test_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_fairness_metrics(y_true, y_pred, sensitive_features, label=None):\n",
        "    mf = MetricFrame(\n",
        "        metrics={\n",
        "            'selection_rate': selection_rate,\n",
        "            'dp_diff': demographic_parity_difference,\n",
        "            'eo_diff': equalized_odds_difference,\n",
        "            'fpr': false_positive_rate,\n",
        "            'fnr': false_negative_rate,\n",
        "            'count': count\n",
        "        },\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        sensitive_features=sensitive_features\n",
        "    )\n",
        "    if label:\n",
        "        print(f\"=== {label} ===\")\n",
        "    print(mf.by_group)\n",
        "    print(\"Overall:\", mf.overall, \"\\n\")\n",
        "    return mf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-Processing\n",
        "compute_fairness_metrics(y_test, preds_lfr, s_test, label=\"LFR + LogisticRegression\")\n",
        "\n",
        "# In-processing\n",
        "compute_fairness_metrics(y_test, pred_gfc.ravel(), s_test, label=\"GerryFairClassifier\")\n",
        "compute_fairness_metrics(y_test, pred_pr.ravel(), s_test, label=\"PrejudiceRemover\")\n",
        "compute_fairness_metrics(y_test, pred_mfc.ravel(), s_test, label=\"MetaFairClassifier\")\n",
        "\n",
        "# Post-processing\n",
        "compute_fairness_metrics(y_test, pred_eop.ravel(), s_test, label=\"EqOddsPostprocessing\")\n",
        "compute_fairness_metrics(y_test, pred_roc.ravel(), s_test, label=\"RejectOptionClassification\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBN9yJ1ExW"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7neYICfm1ExW"
      },
      "outputs": [],
      "source": [
        "def create_model(seed):\n",
        "    tf.random.set_seed(seed)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=22, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Neural Network': create_model(random_seed)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "bSyQkZB91ExX",
        "outputId": "166c5665-77b9-4e4b-d2b7-e20eef41a9d7"
      },
      "outputs": [],
      "source": [
        "metrics = []\n",
        "predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name in ['Linear Regression', 'Decision Tree', 'Naive Bayes', 'XGBoost', 'KNN']:\n",
        "        model.fit(X_train_full, y_train_full)\n",
        "    elif name in ['Neural Network']:\n",
        "        model.fit(X_train_full, y_train_full, epochs=15, batch_size=64, validation_split=0.2)\n",
        "    else:\n",
        "        print(\"Error in Models!\"); break\n",
        "\n",
        "    y_pred = model.predict(X_test_full)\n",
        "\n",
        "    if name in ['Linear Regression', 'XGBoost', 'Neural Network']:\n",
        "        y_pred = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    accuracy = round(accuracy_score(y_test_full, y_pred), 3)\n",
        "    precision = round(precision_score(y_test_full, y_pred), 3)\n",
        "    recall = round(recall_score(y_test_full, y_pred), 3)\n",
        "    f1 = round(f1_score(y_test_full, y_pred), 3)\n",
        "    roc_auc = round(roc_auc_score(y_test_full, y_pred), 3)\n",
        "\n",
        "    metrics.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-score': f1,\n",
        "        'ROC AUC': roc_auc\n",
        "    })\n",
        "    predictions[name] = y_pred\n",
        "\n",
        "metrics = pd.DataFrame(metrics)\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Linear Regression' : predictions['Linear Regression'],\n",
        "    'Decision Tree' : predictions['Decision Tree'],\n",
        "    'Naive Bayes' : predictions['Naive Bayes'],\n",
        "    'XGBoost' : predictions['XGBoost'],\n",
        "    'kNN' : predictions['KNN'],\n",
        "    'Neural Network' : predictions['Neural Network']\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GybD2fNN1Exa"
      },
      "source": [
        "## Fairness Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ws-DQHm1Exa"
      },
      "source": [
        "#### **3.1 Demographic Parity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCdj5tff1Exb"
      },
      "outputs": [],
      "source": [
        "sensitive_features = [' Sex_encoded', ' Age Range_encoded', ' Citizenship_encoded', ' Protected category_encoded']\n",
        "non_sensitive_features = ['Technical Skills', 'Comunication', 'Maturity', 'Dynamism', 'Mobility',\n",
        "       'English', ' Study area_encoded', ' Study Title_encoded', ' Years Experience_encoded', ' Sector_encoded', ' Job Family Hiring_encoded',\n",
        "       ' Job Title Hiring_encoded', ' Overall_encoded', ' Years Experience.1_encoded',' Minimum Ral_encoded', ' Ral Maximum_encoded',\n",
        "       ' Study Level_encoded', 'Current Ral_encoded', 'Expected Ral_encoded']\n",
        "models_list = [model for model in models]\n",
        "tolerance = 0.15\n",
        "significance_level = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzubVOQ61Exb"
      },
      "outputs": [],
      "source": [
        "def calculate_demographic_parity(predictions, sensitive_attribute, name, significance_level, tolerance, activate_check=False):\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'predictions': predictions,\n",
        "        'sensitive_attribute': sensitive_attribute\n",
        "    })\n",
        "    prop = df.groupby('sensitive_attribute')['predictions'].mean()\n",
        "    \n",
        "    if activate_check:\n",
        "        print(f\"===\\n{name}\\n{prop}\")\n",
        "\n",
        "    if prop.shape[0] == 2:\n",
        "        return 'T' if (prop.max() - prop.min()) <= tolerance else False\n",
        "    else:\n",
        "        contingency_table = pd.crosstab(df['predictions'], df['sensitive_attribute'])\n",
        "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "        if activate_check and (expected < 5).any():\n",
        "            print(f\"Sparse contingency for {name}\")\n",
        "                \n",
        "        return 'T' if p > significance_level else False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "dYqVcPet1Exb",
        "outputId": "6422f8a3-7d22-4ab2-d8b8-867596fc79dc"
      },
      "outputs": [],
      "source": [
        "table = []\n",
        "for model in models:\n",
        "    row = []\n",
        "    for sensitive_feature in sensitive_features:\n",
        "        result = calculate_demographic_parity(predictions[model], X_test_full[sensitive_feature], sensitive_feature, significance_level, tolerance, activate_check=True)\n",
        "        row.append(result)\n",
        "    table.append(row)\n",
        "sf_df = pd.DataFrame(table, index = models_list, columns=sensitive_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ1zcOEM1Exb"
      },
      "source": [
        "#### **3.2 Equalized odds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXbOhlKv1Exb"
      },
      "outputs": [],
      "source": [
        "def calculate_equalized_odds(predictions, true_labels, sensitive_attribute, name, tolerance, activate_check=False):\n",
        "    df = pd.DataFrame({\n",
        "        'predictions': predictions,\n",
        "        'true_labels': true_labels,\n",
        "        'sensitive_attribute': sensitive_attribute\n",
        "    })\n",
        "    tprs, fprs = [], []\n",
        "    for _, group_df in df.groupby('sens'):\n",
        "        tn, fp, fn, tp = confusion_matrix(group_df['true_labels'], group_df['predictions'], labels=[0, 1]).ravel()\n",
        "        tprs.append(tp / (tp + fn) if tp + fn != 0 else 0)\n",
        "        fprs.append(fp / (fp + tn) if fp + tn != 0 else 0)\n",
        "\n",
        "    max_tpr_diff = max(tprs) - min(tprs)\n",
        "    max_fpr_diff = max(fprs) - min(fprs)\n",
        "\n",
        "    if activate_check:\n",
        "            print(f\"===\\n{name}\\nMax FPR diff: {max_fpr_diff}\\nMax TPR diff: {max_tpr_diff}\")\n",
        "\n",
        "    return 'T' if (max_tpr_diff <= 2 * tolerance and max_fpr_diff <= 2 * tolerance) else False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "9KSoLzk21Exb",
        "outputId": "6f420612-90e8-4537-f6fb-43add15ddac2"
      },
      "outputs": [],
      "source": [
        "table = []\n",
        "for model in models:\n",
        "    row = []\n",
        "    for sensitive_feature in sensitive_features:\n",
        "        result = calculate_equalized_odds(predictions[model], y_test_full, X_test_full[sensitive_feature], sensitive_feature, tolerance, activate_check=False)\n",
        "        row.append(result)\n",
        "    table.append(row)\n",
        "sf_df = pd.DataFrame(table, index = models_list, columns=sensitive_features)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
